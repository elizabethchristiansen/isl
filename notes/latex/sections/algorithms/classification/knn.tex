\subsection{K-Nearest Neighbour Classifier}

The K-NN algorithm seeks to classify an observation by setting its class to the majority class the $K$ closest known observations. Specifically, K-NN provides a way to estimate class probabilities by relating them to the proportion of each class in the neighbourhood:

$$ Pr(Y=j|X=x_{0}) = \frac{1}{K} \sum_{i\in \mathcal{N}_{0}} I(y_{i} = j) $$

Where $\mathcal{N}_{0}$ is the set of the $K$ nearest data points, $I(...)$ is an indicator variable which equals 1 if true and 0 otherwise. For K-NN, 'nearest' can mean anything as long as its a consistent measure.

With these class probabilities, we can follow the Bayes classifier and set the unknown class to the class $j$ which maximizes $Pr(...)$.
