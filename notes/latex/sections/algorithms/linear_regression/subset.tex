\subsection{Subset Selection}

Subset selection aims to improve the regression by reducing the number of predictors $p$ that need to be considered and therefore makes our dataset $n$ go further (as in, the dataset has a limited amount of information so by reducing $p$ we have more information to accurately predict the remaining predictors). Because of this, subset selection can be used for problems where $p>n$ or $p\approx n$ and should discover the maximum number of useful predictors given $n$ (or if $n$ is large enough, then it will discover all useful predictors). There are three types of subset selection: \textbf{best subset selection}, \textbf{forward stepwise selection} and \textbf{backward stepwise selection}. The latter two can also be combined in to a \textbf{hybrid stepwise selection}.

\subsubsection{Best Subset Selection}

Best subset selection involves fitting a separate least squares regression (the high-level algorithm can be applied to any model however, for example logistic regression) for each possible combination of the $p$ predictors which obviously scales very inefficiently as $p$ increases. The algorithm selects the best model $M_{k}$ for $k=0,...,p$ where $M_{k}$ is the best performing model, scored with $RSS$ or equivalently $R^{2}$, that contains $k$ predictors. With these best models selected, the overall best model is selected from these $p$ models using \textbf{cross-validation}, $C_{p}$, \textbf{AIC}, \textbf{BIC} or adjusted $R^{2}$. We use $RSS$ within each $M_{k}$ as it allows us to grade the models however $RSS$ will decrease ($R^{2}$ will increase) as $k$ increases and so we can't use it to compare across $M_{k}$s as it would always select for the models with more predictors. This is why we use the other measures when selecting the optimal $p$ value.

Best subset selection allows us not only to find the most optimal model but it also allows us to work out which of the $p$ predictors provide useful information for the regression (which predictors does the response depend on). As mentioned before it also allows us to find the predictors that give the most information when we have a limited data set ($n\approx p$) and need to drop predictors to find a closed solution. The obvious downside with this is the massive computing requirements, in general there are $2^{p}$ different models that must be considered. \textbf{Stepwise selection} is a solution to this.

\subsubsection{Forward Stepwise Selection}

Rather than considering all possible combinations of predictors $p$ we instead consider the highest performing predictor at each step and add that to the model. We start with a \textbf{null model}, $M_{0}$ containing no predictors and for $k=0,...,p-1$ we define $M_{k+1}$ which is the model $M_{k}$ with the best performing predictor not already used added to the model, scored using $RSS$ and $R^{2}$. In other words at $k=0$ we select the best performing predictor and create a new model $M_{1}$ which has exactly one predictor (the best performing one). At $k=1$ we test each remaining predictor with the model $M_{1}$ (one at a time, they are not tested together in the same model) and select the new best predictor, adding that to the model $M_{1}$ (which becomes $M_{2}$). We then repeat this until $M_{p}$ is defined. With all $M_{k}$s defined, we select the best one with the same metrics as best subset selection.

This approach is considerably more computationally feasible than best subset selection, requiring only $1+\sum^{p-1}_{k=0} (p-k)$ models (as opposed to $2^{p}$). The downside is that we do not consider every possible combination of predictors and thus we have no guarantee that the best combination will be found (for example, if we have four predictors the optimal combination is 2 and 3, but on step $k=0$ we find the best predictor to be 4, we will always have 4 in our final model which we can see doesn't appear in the optimal model). As with best subset selection, forward stepwise selection can be used with $n<p$ however it is not possible to evaluate models $M_{k}$ for $k\ge n$.

\subsubsection{Backward Stepwise Selection}

Follows a very similar algorithm to forward stepwise selection but instead starts with the fully fitted model, $M_{p}$, and removes a predictor at each step by testing models $M_{p-1}$ with each predictor removed, continuing until the null model $M_{0}$ is found. Unlike the previous two methods, backward stepwise selection requires that $n>p$.

\subsubsection{Hybrid Stepwise Selection}

One can combine forward and backward stepwise selection in to a hybrid solution: at each step perform forward stepwise selection and once the optimal model is obtained for each $k$, check if model performance can be improved by removing any one variable - perform backward stepwise selection. If model performance cannot be improved then move on to the next model $M_{k+1}$ and if performance can be improved, remove that variable and perform forward stepwise selection again for model $M_{k}$. If a cycle is found (each time a variable is added a variable is removed and eventually a cycle forms) then either break it manually or conclude that the best subset has been found.
