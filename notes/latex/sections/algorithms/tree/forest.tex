\subsection{Random Forest}

A random forest is very similar to the bagging approach. One problem with bagging is that for correlated trees - trees that are very similar - the reduction in variance may not be as great as desired. This could arise if there is one very dominant predictor. In such a case, almost all trees that contained that predictor would split on that predictor as the root which means most trees would look pretty similar. Random forest gets around this by randomly selecting a subset $m$, $m<p$, of predictors that are available to be split on on each split. This means that no matter how dominant a predictor is there will always be variation in the bag $B$. Usually, $m$ is set such that $m=\sqrt(p)$. Doing this has the effect of \textit{decoupling} the trees in the bag which means the variance reduction in cases where there is a dominant variable or similar is maintained.
