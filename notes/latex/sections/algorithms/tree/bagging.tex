\subsection{Bagging}

Bagging is the process of applying the bootstrap method to decision trees. As per bootstrapping, we generate $B$ datasets by randomly drawing from our original dataset with replacement and we usually draw the same number of elements that our original dataset had. These $B$ datasets are not necessarily unique but will usually be so. We train a deep decision tree on each dataset without pruning. This is the bagged model. We can then make predictions using our bag of $B$ trees by performing the inference with each tree individually and averaging the results. While normally a deep decision tree would have very high variance, it can be shown that the variance of a set of $n$ variables that have their own variance $\sigma$ is $\frac{\sigma}{n}$ (this is, of course, only if these are uncorrelated variables - this is not always the case as we will see next). As such, if $B$ (equal to $n$ here) is large our resultant bagged model will have low variance despite having deep trees. This, in essence, avoids the overfitting problems with deep trees.

We can even generate a valid estimate for the test error with very little added computation. Normally we would need to perform cross validation to estimate test error but due to how bootstrap datasets are drawn, it can be shown that each dataset will use, on average, two-thirds of the original dataset. This means, that for any observation, there exists $\frac{B}{3}$ generated trees that were not generated with that observation. In essence, this acts as a test set for that observation, much like cross validation. You can calculate the average error for that observation across those trees and then perform the same process across all observations, averaging that result also. The result will be a valid estimator for the test set MSE. This is known as \textbf{out-of-bag error}.

While bagging is less interpretable than a single decision tree, one can still generate a measure of variable importance. We could calculate the average MSE reduction for a split on a given variable across our entire bag, or equivalently calculate the average Gini index for classification problems. While our bagged model may not be as graphically interpretable we can still give a measure of which variables are most important or offer the most gains.
