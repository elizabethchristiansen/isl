\textbf{Input Variables:} Input variables are the set of data points that are fed in to the model and are generally referred to as $X$. Each individual data point is usually indexed as $x_{i}$ for the $i^{th}$ data point, each of which can be a vector indexed with $j$, $x_{ij}$, for the $j^{th}$ element of the $i^{th}$ data point. Input variables can also be referred to as the \textbf{predictors}, \textbf{independent variables} or the \textbf{features}.

\textbf{Output Variables:} Output variables are the result or outcome of a given event and are (we hope!) related to a corresponding set of input variables. Output variables are referred to as $Y$ and each individual point can be indexed as $y_{i}$ for the $i^{th}$ data point. As with input variables, each data point could also be a vector indexed with $j$, $y_{ij}$. Input and output variables are coupled as the data point $x_{i}$ is used to predict the output $y_{i}$ and are sometimes represented as $((x_{1},y_{1}), (x_{2},y_{2}),...,(x_{n},y_{n}))$. Output variables are also known as the \textbf{dependent variable} or the \textbf{response}. 

\textbf{Statistical Learning:} Statistical learning (and, in general, machine learning) aims to take $(x_{i},y_{i})$ pairs and find the relationship between them, expressed formally as:

$$ Y = f(X) + \epsilon $$

Where $\epsilon$ is the irreducible error and $f$ represents the \textbf{function}, \textbf{relationship} or \textbf{mapping} between $X$ and $Y$. Given this, our aim is to learn $f$. There are two types of error present here, the \textbf{irreducible error} and the \textbf{reducible error}.

\textbf{Irreducible Error:} Irreducible error, $\epsilon$, represents error we cannot account for (and therefore not reduce) such as our data not being representative of the true relationship or inherent noise in the relationship. In theory you could reduce this error, in certain cases, but you would have to go outside the scope of the problem (collect more data, understand more about the underlying relationship) which is not always feasible.

\textbf{Reducible Error:} Reducible error is found within $f$ and is the difference between the true $f$ and our predicted $f$, $\hat{f}$. It can arise from $\hat{f}$ being the wrong form (a linear $\hat{f}$ approximating a non-linear $f$), being too flexible or not flexible enough, errors in the learning algorithm (rounding errors, approximations for computational tractability) and a number of other ways. Importantly, reducible error is named as so because we can take steps to reduce this error, relying on our domain knowledge or experience in selecting the proper approach.

\textbf{Supervised vs Unsupervised Learning:} In supervised learning each $x_{i}$ has a corresponding $y_{i}$ and thus there is a complete set of responses for all predictors. We therefore aim to learn the relationship between the two. Unsupervised learning is different in that we do not have corresponding $y_{i}$ for each $x_{i}$. We lack the data to learn $\hat{f}$ and either $y_{i}$ do not exist or are otherwise unobtainable so there is no approach to tackle $\hat{f}$. In such a case we instead focus on learning relationships \textit{within} the set of predictors such as relative differences between each data point. Semi-supervised learning also exists where we have a set of $y_{i}$ for $i<n$ where $n$ is our number of data points. In such a case, we can apply supervised learning for the set of predictors that have responses and incorporate the remaining predictors with another method.

\textbf{Flexibility vs Interpretability:} The flexibility of a model is proportional to how many degrees of freedom it contains (how many learnable parameters it contains) and flexible models tend to learn complex relationships better. Interpretable models are models that allow us to understand the learned relationships in the data, for example learning the values of the parameters in linear regression lets us understand the influence of each predictor on the response. These two concepts are usually opposed in that by increasing one we reduce the other. Approaches that can accurately model complex behaviour (flexible) tend to be very hard to interpret for a human (multi layer neural nets) and vice versa.

\textbf{Prediction vs Inference:} When undertaking a statistical learning task you must decide what the goal is, be that prediction or inference. Prediction focuses on the product of the statistical learning, namely the learned $Y$ values, and does not care for how they were obtained or the particular form of $\hat{f}$. Due to this, prediction places little emphasis on interpretability and prefers as accurate as model as possible. Inference on the other hand does not care so much about what the predicted $Y$ values are and focuses more on the relationship between $X$ and $Y$, $\hat{f}$, and how each predictor influences the outcome. It therefore concerns itself a great deal more with interpretability, potentially at the expense of (some) flexibility. Prediction would be using the weather today to predict whether it will rain tomorrow, inference would be working out what aspects of the weather today, and with what importance, will influence whether it rains tomorrow.

\textbf{Parametric vs Non-Parametric Approaches:} When approaching a statistical learning problem you must decide how you will learn $\hat{f}$. A parametric approach seeks to simplify the problem to learning a set of parameters that define the behaviour of $\hat{f}$ by selecting a form for $\hat{f}$ manually and parametrising it. A common example would be a linear equation for $\hat{f}$:

$$ \hat{f}(x_{i}) = A x_{i1} + B x_{i2} + ... + C x_{in} + D $$

Where $A$, $B$, $C$ and $D$ are the parameters. A non-parametric approach would instead try to learn the form of $\hat{f}$ directly. Parametric approaches tend to be more interpretable and easier to train, requiring less data and less computation. Conversely, non-parametric approaches require considerably more data and computational power and are usually less interpretable but offer gains in accuracy, are more flexible (can better model highly complex relationships) and reduce the need to know what the correct form for $\hat{f}$ is beforehand.

\textbf{Over/Under Fitting:} Overfitting refers to a $\hat{f}$ which models the training data too accurately, to the point where it is modelling random noise in the dataset as if it were part of the true relationship in the data. The knock on effect of this is that when you move to a test (or production) data set where that random noise does not exist, you are predicting outcomes based on false relationships. Overfitting can come from an overly flexible model (too many degrees of freedom for the true relationship so some get filled by noise) or a model trained for too long. Underfitting refers to the opposite problem where the model does not (or cannot, if the model is too simplistic) learn enough of the true relationship in the data and therefore produces incorrect predictions.

\textbf{Regression vs Classification:} Regression problems involve response variables that are numeric and continuous whereas classification problems involve response variables that are discrete and fall in to categories or classes. In either case, the form of the predictor variables usually assumes the same form as the response but this is not always the case and is not particularly important either way.

\textbf{Bias:} Bias is a type of error that is introduced when approximating complex real world problems with simpler models. Bias is a \textbf{systematic} error inherent in the model and is defined as the difference between the expected sample value and the population (true) value. In the case of population statistics, bias is defined as the difference between the expectation (average) of the sample statistic (mean, variance) and the population statistic. In general, more flexible models reduce the bias in the system as they better fit the underlying relationship. Important to note here, bias is not a measure of how wrong the value is \textit{in practice} but rather whether the model/equation is systematically over/under estimating the true value. For example, with the sample variance, defined as $S^{2} = \frac{1}{n} \sum^{n}_{i=1} (X_{i} - \bar{X})^{2}$, the expectation value $E[S^{2}]$ will equal $(1-\frac{1}{n})\sigma^{2}$ which is less than the true statistic $\sigma^{2}$ (unless $n$ goes to infinity), which means that $S^{2}$ is a biased estimator. This can be fixed by setting $\frac{1}{n} \to \frac{1}{n-1}$ in the definition of $S^{2}$.

\textbf{Variance:} Variance is mathematically understood as a measure of squared deviation from the mean and acts as a metric for 'spread'. Variance can be understood as the amount the model's predictions would change if we estimated it with a different training set - how 'stable' or generalized is our solution or does it depend heavily on each individual data point for its shape. Flexible models tend to introduce variance as the model's expressiveness allows for small changes in the training set to lead to large changes in the learned model.

\textbf{The Bias-Variance Trade-Off:} Flexible models reduce bias but increase variance, restrictive models do the opposite. The bias-variance trade-off is the balancing act between flexibility and accuracy that minimizes both measures. Specifically, the expected test MSE can be decomposed in to three parts: $E[y_{0}-\hat{f}(x_{0})] = Var(\hat{f}(x_{0})) + [Bias(\hat{f}(x_{0}))]^{2} + Var(\epsilon)$. We want to minimize the test MSE and looking at this equation, $\epsilon$ cannot be reduced and so we must reduce the variance and (square) bias. Though as we just saw, we struggle to reduce one without increasing the other and so we must make a trade-off.

\textbf{$l_{1}$ Norm}: The $l_{1}$ norm of a vector, $v$, of length $n$ is defined as:

$$ ||v||_{1} = \sum^{n}_{i=1}|v_{i}| $$

Where $|x|$ is the absolute value of $x$. When applied as regularization, the $l_{1}$ norm tends to set small weights equal to zero.

\textbf{$l_{2}$ Norm}: The $l_{2}$ norm of a vector, $v$, of length $n$ is defined as:

$$ ||v||_{2} = \sqrt{\sum^{n}_{i=1}v_{i}^{2}} $$

The $l_{2}$ norm measures the distance of $v$ from zero. The $l_{2}$ norm is also known as the \textbf{Euclidean norm}. When applied as regularization, the $l_{2}$ norm tends to penalize large weights and tends all weights towards zero (but usually not equal to).

\textbf{Kernel Functions}: A kernel function is a function which defines an analogous 'inner product' for objects other than vectors. In other words, a kernel function is a generalization of the inner product and thus it quantifies the similarity of two observations. A kernel function is therefore a function of two objects (e.g. vectors) and outputs a similarity measure which can be tailored to the problem. A linear kernel might look something like $K(x_{i},x_{i'})=\sum_{j=1}x_{ij}x_{i'j}$ while the \textbf{radial kernel} may look like $K(x_{i},x_{i'})=exp(-\gamma \sum_{j=1}(x_{ij}-x_{i'j})^{2})$ where the radial kernel is good at generating circular decision boundaries. The radial kernel is of particular note, it is a commonly used kernel function. It works because observations far away from the test point ($(x_{ij}-x_{i'j})^{2}$ is large) will produce very small similarity measures (small $K$) and so it has a \textbf{local behaviour} which allows it to 'wrap around' groups of observations.
